{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#TEXT CATEGORY\n",
        "def classify_text(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    sports_keywords = {\n",
        "        \"athlete\", \"coach\", \"referee\", \"umpire\", \"tournament\", \"league\", \"championship\",\n",
        "        \"match\", \"competition\", \"victory\", \"defeat\", \"training\", \"practice\", \"stadium\",\n",
        "        \"arena\", \"court\", \"field\", \"track\", \"swimming\", \"running\", \"jumping\", \"throwing\",\n",
        "        \"cycling\", \"boxing\", \"wrestling\", \"gymnastics\", \"skiing\", \"snowboarding\", \"surfing\",\n",
        "        \"volleyball\", \"handball\", \"rugby\", \"cricket\", \"badminton\", \"table tennis\", \"golf\",\n",
        "        \"motorsport\", \"racing\", \"fishing\", \"hunting\", \"climbing\", \"hiking\", \"camping\",\n",
        "        \"outdoors\", \"adventure\", \"extreme sports\", \"fitness\", \"workout\", \"exercise\",\n",
        "        \"nutrition\", \"injury\", \"recovery\", \"physiotherapy\", \"sports medicine\",\n",
        "        \"sports science\", \"sports psychology\", \"sports management\", \"sports marketing\",\n",
        "        \"sports broadcasting\", \"sports photography\", \"sports writing\", \"sports history\",\n",
        "        \"sports statistics\", \"fantasy sports\", \"gambling\", \"betting\", \"esports\", \"gaming\"\n",
        "    }\n",
        "\n",
        "    technology_keywords = {\n",
        "        \"developer\", \"programmer\", \"coder\", \"engineer\", \"software\", \"web\", \"mobile\",\n",
        "        \"cloud\", \"data\", \"machine\", \"learning\", \"neural\", \"network\", \"computer\", \"vision\",\n",
        "        \"robotics\", \"automation\", \"internet\", \"blockchain\", \"cryptocurrency\", \"virtual\",\n",
        "        \"augmented\", \"cybersecurity\", \"security\", \"information\", \"technology\", \"telecommunications\",\n",
        "        \"wireless\", \"smartphone\", \"tablet\", \"laptop\", \"desktop\", \"server\", \"database\",\n",
        "        \"algorithm\", \"api\", \"framework\", \"library\", \"git\", \"docker\", \"kubernetes\", \"aws\",\n",
        "        \"azure\", \"cloud\", \"saas\", \"paas\", \"iaas\", \"fintech\", \"edtech\", \"healthtech\", \"biotech\",\n",
        "        \"quantum\", \"ethical\", \"privacy\", \"encryption\", \"firewall\", \"malware\", \"hacking\",\n",
        "        \"testing\", \"devops\", \"open\", \"source\", \"ux\", \"ui\", \"cto\", \"cio\"\n",
        "    }\n",
        "\n",
        "    politics_keywords = {\n",
        "        \"government\", \"election\", \"parliament\", \"senate\", \"congress\", \"president\",\n",
        "        \"minister\", \"party\", \"policy\", \"law\", \"bill\", \"vote\", \"campaign\", \"democracy\",\n",
        "        \"republic\", \"political\", \"politics\", \"politician\", \"treaty\", \"summit\", \"diplomacy\",\n",
        "        \"legislature\"\n",
        "    }\n",
        "\n",
        "    finance_keywords = {\n",
        "        \"finance\", \"financial\", \"economy\", \"economic\", \"market\", \"stock\", \"bond\",\n",
        "        \"investment\", \"investor\", \"currency\", \"exchange\", \"bank\", \"loan\", \"credit\",\n",
        "        \"debt\", \"income\", \"revenue\", \"profit\", \"loss\", \"tax\", \"budget\", \"audit\",\n",
        "        \"accounting\", \"insurance\", \"mortgage\", \"recession\", \"inflation\", \"interest\",\n",
        "        \"gdp\", \"fiscal\", \"monetary\"\n",
        "    }\n",
        "\n",
        "    health_keywords = {\n",
        "        \"health\", \"medical\", \"disease\", \"patient\", \"doctor\", \"hospital\", \"therapy\",\n",
        "        \"treatment\", \"symptom\", \"diagnosis\", \"illness\", \"wellness\", \"nutrition\",\n",
        "        \"exercise\", \"fitness\", \"medicine\", \"healthcare\", \"public\", \"epidemic\", \"pandemic\",\n",
        "        \"virus\", \"bacteria\", \"mental\", \"psychology\", \"pharmacy\", \"drug\", \"vaccine\",\n",
        "        \"clinic\", \"surgery\", \"recovery\"\n",
        "    }\n",
        "\n",
        "    entertainment_keywords = {\n",
        "        \"movie\", \"film\", \"show\", \"television\", \"series\", \"actor\", \"actress\", \"director\",\n",
        "        \"producer\", \"music\", \"song\", \"album\", \"artist\", \"concert\", \"festival\", \"theater\",\n",
        "        \"play\", \"comedy\", \"drama\", \"action\", \"thriller\", \"horror\", \"romance\",\n",
        "        \"animation\", \"documentary\", \"celebrity\", \"star\", \"award\", \"gala\", \"premiere\",\n",
        "        \"box\", \"streaming\", \"netflix\", \"hulu\", \"disney\", \"amazon\", \"youtube\", \"tiktok\",\n",
        "        \"instagram\", \"social\", \"media\", \"pop\", \"culture\", \"magazine\", \"interview\",\n",
        "        \"review\", \"critic\", \"fan\", \"fandom\", \"cosplay\", \"gaming\", \"esports\", \"twitch\"\n",
        "    }\n",
        "\n",
        "    scores = {\n",
        "        \"sports\": 0,\n",
        "        \"politics\": 0,\n",
        "        \"technology\": 0,\n",
        "        \"health\": 0,\n",
        "        \"finance\": 0,\n",
        "        \"entertainment\": 0\n",
        "    }\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in sports_keywords:\n",
        "            scores[\"sports\"] += 1\n",
        "        if word in politics_keywords:\n",
        "            scores[\"politics\"] += 1\n",
        "        if word in technology_keywords:\n",
        "            scores[\"technology\"] += 1\n",
        "        if word in health_keywords:\n",
        "            scores[\"health\"] += 1\n",
        "        if word in finance_keywords:\n",
        "            scores[\"finance\"] += 1\n",
        "        if word in entertainment_keywords:\n",
        "            scores[\"entertainment\"] += 1\n",
        "\n",
        "    max_category = max(scores, key=scores.get)\n",
        "\n",
        "    if scores[max_category] == 0:\n",
        "        return \"unknown\", scores\n",
        "    return max_category, scores\n",
        "\n",
        "\n",
        "print(\"Enter your text document:\")\n",
        "user_text = input(\"\\nYour document: \")\n",
        "\n",
        "category, counts = classify_text(user_text)\n",
        "print(f\"Classified as: {category}\")\n",
        "print(\"Keyword counts per category:\")\n",
        "for cat, count in counts.items():\n",
        "    print(f\"- {cat.capitalize()}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKjP5ONjFP3Y",
        "outputId": "143c2d49-13da-434e-f52f-b58b9e1b3b2b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text document:\n",
            "\n",
            "Your document: The proposed budgetary cuts sparked immediate outrage across the political spectrum, with opposition leaders decrying the potential impact on essential public services. Meanwhile, bond yields saw a volatile week, reflecting investor unease about the nation's rising debt-to-GDP ratio. Analysts debated whether the government's fiscal austerity measures, championed by the ruling party as crucial for long-term economic stability, would be enough to assuage the credit rating agencies or if political pressures would ultimately force a more expansive, and potentially inflationary, spending package. The upcoming elections are widely seen as a referendum on the current administration's handling of both the national finances and the broader geopolitical climate, which continues to influence global commodity prices and, by extension, domestic inflation.\n",
            "Classified as: finance\n",
            "Keyword counts per category:\n",
            "- Sports: 0\n",
            "- Politics: 3\n",
            "- Technology: 0\n",
            "- Health: 1\n",
            "- Finance: 5\n",
            "- Entertainment: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WITH PREPROCESSING\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize # Import word_tokenize\n",
        "\n",
        "# Download necessary NLTK data if you haven't already\n",
        "# Run these two lines ONCE if you get errors about missing 'wordnet' or 'punkt'\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    nltk.download('punkt_tab')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger_eng')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get the correct WordNet POS tag for lemmatization\n",
        "# NLTK lemmatizer needs a POS tag (e.g., 'n' for noun, 'v' for verb)\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag for lemmatization\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if not found\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Punctuation Removal (keep spaces)\n",
        "    # This pattern matches any character that is NOT a letter (a-z) or a digit (0-9) or a space.\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenization: Split text into words\n",
        "    words = word_tokenize(text) # Use word_tokenize for better tokenization\n",
        "\n",
        "    # 4. Lemmatization\n",
        "    # Apply lemmatization to each word, trying to get its POS tag for better accuracy\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "    # Join the lemmatized words back into a single string for consistency with later splitting,\n",
        "    # or return as a list if you prefer to iterate over the list directly in classify_text\n",
        "    # For this current setup, it's easier to return as a list of words.\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "# TEXT CATEGORY CLASSIFICATION FUNCTION\n",
        "def classify_text(text_words): # Now expects a list of pre-processed words\n",
        "    sports_keywords = {\n",
        "        \"athlete\", \"coach\", \"referee\", \"umpire\", \"tournament\", \"league\", \"championship\",\n",
        "        \"match\", \"competition\", \"victory\", \"defeat\", \"training\", \"practice\", \"stadium\",\n",
        "        \"arena\", \"court\", \"field\", \"track\", \"swimming\", \"running\", \"jumping\", \"throwing\",\n",
        "        \"cycling\", \"boxing\", \"wrestling\", \"gymnastics\", \"skiing\", \"snowboarding\", \"surfing\",\n",
        "        \"volleyball\", \"handball\", \"rugby\", \"cricket\", \"badminton\", \"table tennis\", \"golf\",\n",
        "        \"motorsport\", \"racing\", \"fishing\", \"hunting\", \"climbing\", \"hiking\", \"camping\",\n",
        "        \"outdoors\", \"adventure\", \"extreme sports\", \"fitness\", \"workout\", \"exercise\",\n",
        "        \"nutrition\", \"injury\", \"recovery\", \"physiotherapy\", \"sports medicine\",\n",
        "        \"sports science\", \"sports psychology\", \"sports management\", \"sports marketing\",\n",
        "        \"sports broadcasting\", \"sports photography\", \"sports writing\", \"sports history\",\n",
        "        \"sports statistics\", \"fantasy sports\", \"gambling\", \"betting\", \"esports\", \"gaming\"\n",
        "    }\n",
        "\n",
        "    technology_keywords = {\n",
        "        \"developer\", \"programmer\", \"coder\", \"engineer\", \"software\", \"web\", \"mobile\",\n",
        "        \"cloud\", \"data\", \"machine\", \"learning\", \"neural\", \"network\", \"computer\", \"vision\",\n",
        "        \"robotics\", \"automation\", \"internet\", \"blockchain\", \"cryptocurrency\", \"virtual\",\n",
        "        \"augmented\", \"cybersecurity\", \"security\", \"information\", \"technology\", \"telecommunications\",\n",
        "        \"wireless\", \"smartphone\", \"tablet\", \"laptop\", \"desktop\", \"server\", \"database\",\n",
        "        \"algorithm\", \"api\", \"framework\", \"library\", \"git\", \"docker\", \"kubernetes\", \"aws\",\n",
        "        \"azure\", \"cloud\", \"saas\", \"paas\", \"iaas\", \"fintech\", \"edtech\", \"healthtech\", \"biotech\",\n",
        "        \"quantum\", \"ethical\", \"privacy\", \"encryption\", \"firewall\", \"malware\", \"hacking\",\n",
        "        \"testing\", \"devops\", \"open\", \"source\", \"ux\", \"ui\", \"cto\", \"cio\"\n",
        "    }\n",
        "\n",
        "    politics_keywords = {\n",
        "        \"government\", \"election\", \"parliament\", \"senate\", \"congress\", \"president\",\n",
        "        \"minister\", \"party\", \"policy\", \"law\", \"bill\", \"vote\", \"campaign\", \"democracy\",\n",
        "        \"republic\", \"political\", \"politics\", \"politician\", \"treaty\", \"summit\", \"diplomacy\",\n",
        "        \"legislature\"\n",
        "    }\n",
        "\n",
        "    finance_keywords = {\n",
        "        \"finance\", \"financial\", \"economy\", \"economic\", \"market\", \"stock\", \"bond\",\n",
        "        \"investment\", \"investor\", \"currency\", \"exchange\", \"bank\", \"loan\", \"credit\",\n",
        "        \"debt\", \"income\", \"revenue\", \"profit\", \"loss\", \"tax\", \"budget\", \"audit\",\n",
        "        \"accounting\", \"insurance\", \"mortgage\", \"recession\", \"inflation\", \"interest\",\n",
        "        \"gdp\", \"fiscal\", \"monetary\"\n",
        "    }\n",
        "\n",
        "    health_keywords = {\n",
        "        \"health\", \"medical\", \"disease\", \"patient\", \"doctor\", \"hospital\", \"therapy\",\n",
        "        \"treatment\", \"symptom\", \"diagnosis\", \"illness\", \"wellness\", \"nutrition\",\n",
        "        \"exercise\", \"fitness\", \"medicine\", \"healthcare\", \"public\", \"epidemic\", \"pandemic\",\n",
        "        \"virus\", \"bacteria\", \"mental\", \"psychology\", \"pharmacy\", \"drug\", \"vaccine\",\n",
        "        \"clinic\", \"surgery\", \"recovery\"\n",
        "    }\n",
        "\n",
        "    entertainment_keywords = {\n",
        "        \"movie\", \"film\", \"show\", \"television\", \"series\", \"actor\", \"actress\", \"director\",\n",
        "        \"producer\", \"music\", \"song\", \"album\", \"artist\", \"concert\", \"festival\", \"theater\",\n",
        "        \"play\", \"comedy\", \"drama\", \"action\", \"thriller\", \"horror\", \"romance\",\n",
        "        \"animation\", \"documentary\", \"celebrity\", \"star\", \"award\", \"gala\", \"premiere\",\n",
        "        \"box\", \"streaming\", \"netflix\", \"hulu\", \"disney\", \"amazon\", \"youtube\", \"tiktok\",\n",
        "        \"instagram\", \"social\", \"media\", \"pop\", \"culture\", \"magazine\", \"interview\",\n",
        "        \"review\", \"critic\", \"fan\", \"fandom\", \"cosplay\", \"gaming\", \"esports\", \"twitch\"\n",
        "    }\n",
        "\n",
        "\n",
        "    scores = {\n",
        "        \"sports\": 0,\n",
        "        \"politics\": 0,\n",
        "        \"technology\": 0,\n",
        "        \"health\": 0,\n",
        "        \"finance\": 0,\n",
        "        \"entertainment\": 0\n",
        "    }\n",
        "\n",
        "    # Iterate through the pre-processed (lemmatized) words\n",
        "    for word in text_words:\n",
        "        if word in sports_keywords:\n",
        "            scores[\"sports\"] += 1\n",
        "        if word in politics_keywords:\n",
        "            scores[\"politics\"] += 1\n",
        "        if word in technology_keywords:\n",
        "            scores[\"technology\"] += 1\n",
        "        if word in health_keywords:\n",
        "            scores[\"health\"] += 1\n",
        "        if word in finance_keywords:\n",
        "            scores[\"finance\"] += 1\n",
        "        if word in entertainment_keywords:\n",
        "            scores[\"entertainment\"] += 1\n",
        "\n",
        "    max_category = max(scores, key=scores.get)\n",
        "\n",
        "    if scores[max_category] == 0:\n",
        "        return \"unknown\", scores\n",
        "    return max_category, scores\n",
        "\n",
        "\n",
        "print(\"Enter your text document:\")\n",
        "user_text = input(\"\\nYour document: \")\n",
        "\n",
        "# Call the pre-processing function first\n",
        "processed_words = preprocess_text(user_text)\n",
        "\n",
        "# Then call the classification function with the processed words\n",
        "category, counts = classify_text(processed_words)\n",
        "\n",
        "print(f\"Classified as: {category}\")\n",
        "print(\"Keyword counts per category:\")\n",
        "for cat, count in counts.items():\n",
        "    print(f\"- {cat.capitalize()}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEc7wGFLGw0_",
        "outputId": "84c44153-48d6-4a77-96c9-58d76a471ecd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your text document:\n",
            "\n",
            "Your document: The proposed budgetary cuts sparked immediate outrage across the political spectrum, with opposition leaders decrying the potential impact on essential public services. Meanwhile, bond yields saw a volatile week, reflecting investor unease about the nation's rising debt-to-GDP ratio. Analysts debated whether the government's fiscal austerity measures, championed by the ruling party as crucial for long-term economic stability, would be enough to assuage the credit rating agencies or if political pressures would ultimately force a more expansive, and potentially inflationary, spending package. The upcoming elections are widely seen as a referendum on the current administration's handling of both the national finances and the broader geopolitical climate, which continues to influence global commodity prices and, by extension, domestic inflation.\n",
            "Classified as: finance\n",
            "Keyword counts per category:\n",
            "- Sports: 0\n",
            "- Politics: 5\n",
            "- Technology: 0\n",
            "- Health: 1\n",
            "- Finance: 7\n",
            "- Entertainment: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Personal observation:**\n",
        "\n",
        "Without performing any pre processing on the data the number of counts for politcs was 3 and fiance was 5 and conclusion was my text classifies as finance, but pre processing is necessary because after doing that the word incresed and so for the code to function correctly for large chunks of data we must always pre process the day."
      ],
      "metadata": {
        "id": "QMPZPN4hHUGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# now making a front end for text classfier\n",
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REckQw5MICwl",
        "outputId": "c371b15a-4bf6-4197-a286-10d0db1cffe2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WITH PREPROCESSING AND GRADIO FRONTEND\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gradio as gr # Import Gradio\n",
        "\n",
        "# Download necessary NLTK data if you haven't already\n",
        "# These lines should ideally be run once, not every time the script executes in production.\n",
        "# For a Gradio app, it's good to ensure they are downloaded.\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get the correct WordNet POS tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag for lemmatization\"\"\"\n",
        "    # pos_tag expects a list of words, so [word]\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN) # Default to noun if not found\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Punctuation Removal (keep spaces)\n",
        "    # This pattern matches any character that is NOT a letter (a-z) or a digit (0-9) or a space.\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenization: Split text into words using NLTK's word_tokenize\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # 4. Lemmatization\n",
        "    # Apply lemmatization to each word, trying to get its POS tag for better accuracy\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "    return lemmatized_words\n",
        "\n",
        "\n",
        "# TEXT CATEGORY CLASSIFICATION FUNCTION\n",
        "def classify_text(text_words): # Now expects a list of pre-processed words\n",
        "    sports_keywords = {\n",
        "        \"athlete\", \"coach\", \"referee\", \"umpire\", \"tournament\", \"league\", \"championship\",\n",
        "        \"match\", \"competition\", \"victory\", \"defeat\", \"training\", \"practice\", \"stadium\",\n",
        "        \"arena\", \"court\", \"field\", \"track\", \"swimming\", \"running\", \"jumping\", \"throwing\",\n",
        "        \"cycling\", \"boxing\", \"wrestling\", \"gymnastics\", \"skiing\", \"snowboarding\", \"surfing\",\n",
        "        \"volleyball\", \"handball\", \"rugby\", \"cricket\", \"badminton\", \"table tennis\", \"golf\",\n",
        "        \"motorsport\", \"racing\", \"fishing\", \"hunting\", \"climbing\", \"hiking\", \"camping\",\n",
        "        \"outdoors\", \"adventure\", \"extreme sports\", \"fitness\", \"workout\", \"exercise\",\n",
        "        \"nutrition\", \"injury\", \"recovery\", \"physiotherapy\", \"sports medicine\",\n",
        "        \"sports science\", \"sports psychology\", \"sports management\", \"sports marketing\",\n",
        "        \"sports broadcasting\", \"sports photography\", \"sports writing\", \"sports history\",\n",
        "        \"sports statistics\", \"fantasy sports\", \"gambling\", \"betting\", \"esports\", \"gaming\"\n",
        "    }\n",
        "\n",
        "    technology_keywords = {\n",
        "        \"developer\", \"programmer\", \"coder\", \"engineer\", \"software\", \"web\", \"mobile\",\n",
        "        \"cloud\", \"data\", \"machine\", \"learning\", \"neural\", \"network\", \"computer\", \"vision\",\n",
        "        \"robotics\", \"automation\", \"internet\", \"blockchain\", \"cryptocurrency\", \"virtual\",\n",
        "        \"augmented\", \"cybersecurity\", \"security\", \"information\", \"technology\", \"telecommunications\",\n",
        "        \"wireless\", \"smartphone\", \"tablet\", \"laptop\", \"desktop\", \"server\", \"database\",\n",
        "        \"algorithm\", \"api\", \"framework\", \"library\", \"git\", \"docker\", \"kubernetes\", \"aws\",\n",
        "        \"azure\", \"cloud\", \"saas\", \"paas\", \"iaas\", \"fintech\", \"edtech\", \"healthtech\", \"biotech\",\n",
        "        \"quantum\", \"ethical\", \"privacy\", \"encryption\", \"firewall\", \"malware\", \"hacking\",\n",
        "        \"testing\", \"devops\", \"open\", \"source\", \"ux\", \"ui\", \"cto\", \"cio\"\n",
        "    }\n",
        "\n",
        "    politics_keywords = {\n",
        "        \"government\", \"election\", \"parliament\", \"senate\", \"congress\", \"president\",\n",
        "        \"minister\", \"party\", \"policy\", \"law\", \"bill\", \"vote\", \"campaign\", \"democracy\",\n",
        "        \"republic\", \"political\", \"politics\", \"politician\", \"treaty\", \"summit\", \"diplomacy\",\n",
        "        \"legislature\"\n",
        "    }\n",
        "\n",
        "    finance_keywords = {\n",
        "        \"finance\", \"financial\", \"economy\", \"economic\", \"market\", \"stock\", \"bond\",\n",
        "        \"investment\", \"investor\", \"currency\", \"exchange\", \"bank\", \"loan\", \"credit\",\n",
        "        \"debt\", \"income\", \"revenue\", \"profit\", \"loss\", \"tax\", \"budget\", \"audit\",\n",
        "        \"accounting\", \"insurance\", \"mortgage\", \"recession\", \"inflation\", \"interest\",\n",
        "        \"gdp\", \"fiscal\", \"monetary\"\n",
        "    }\n",
        "\n",
        "    health_keywords = {\n",
        "        \"health\", \"medical\", \"disease\", \"patient\", \"doctor\", \"hospital\", \"therapy\",\n",
        "        \"treatment\", \"symptom\", \"diagnosis\", \"illness\", \"wellness\", \"nutrition\",\n",
        "        \"exercise\", \"fitness\", \"medicine\", \"healthcare\", \"public\", \"epidemic\", \"pandemic\",\n",
        "        \"virus\", \"bacteria\", \"mental\", \"psychology\", \"pharmacy\", \"drug\", \"vaccine\",\n",
        "        \"clinic\", \"surgery\", \"recovery\"\n",
        "    }\n",
        "\n",
        "    entertainment_keywords = {\n",
        "        \"movie\", \"film\", \"show\", \"television\", \"series\", \"actor\", \"actress\", \"director\",\n",
        "        \"producer\", \"music\", \"song\", \"album\", \"artist\", \"concert\", \"festival\", \"theater\",\n",
        "        \"play\", \"comedy\", \"drama\", \"action\", \"thriller\", \"horror\", \"romance\",\n",
        "        \"animation\", \"documentary\", \"celebrity\", \"star\", \"award\", \"gala\", \"premiere\",\n",
        "        \"box\", \"streaming\", \"netflix\", \"hulu\", \"disney\", \"amazon\", \"youtube\", \"tiktok\",\n",
        "        \"instagram\", \"social\", \"media\", \"pop\", \"culture\", \"magazine\", \"interview\",\n",
        "        \"review\", \"critic\", \"fan\", \"fandom\", \"cosplay\", \"gaming\", \"esports\", \"twitch\"\n",
        "    }\n",
        "\n",
        "    all_keywords = {\n",
        "        \"sports\": sports_keywords,\n",
        "        \"politics\": politics_keywords,\n",
        "        \"technology\": technology_keywords,\n",
        "        \"health\": health_keywords,\n",
        "        \"finance\": finance_keywords,\n",
        "        \"entertainment\": entertainment_keywords\n",
        "    }\n",
        "\n",
        "    scores = {\n",
        "        \"sports\": 0,\n",
        "        \"politics\": 0,\n",
        "        \"technology\": 0,\n",
        "        \"health\": 0,\n",
        "        \"finance\": 0,\n",
        "        \"entertainment\": 0\n",
        "    }\n",
        "\n",
        "    # Iterate through the pre-processed (lemmatized) words\n",
        "    for word in text_words:\n",
        "        for category_name, keywords_set in all_keywords.items():\n",
        "            if word in keywords_set:\n",
        "                scores[category_name] += 1\n",
        "\n",
        "    max_category = max(scores, key=scores.get)\n",
        "\n",
        "    if scores[max_category] == 0:\n",
        "        return \"unknown\", scores\n",
        "    return max_category, scores\n",
        "\n",
        "# Define the Gradio interface function\n",
        "def categorize_document(document_text):\n",
        "    # 1. Preprocess the input text\n",
        "    processed_words = preprocess_text(document_text)\n",
        "\n",
        "    # 2. Classify the preprocessed words\n",
        "    category, counts = classify_text(processed_words)\n",
        "\n",
        "    # 3. Format the output for display in Gradio\n",
        "    output_string = \"\"\n",
        "    if category == \"unknown\":\n",
        "        output_string += \"**Classification Result:** Your text category is <span style='color: gray; font-weight: bold;'>UNKNOWN</span> (no specific keywords found).\\n\\n\"\n",
        "    else:\n",
        "        output_string += f\"**Classification Result:** Your text is a <span style='color: blue; font-weight: bold;'>{category.upper()}</span> category.\\n\\n\"\n",
        "\n",
        "    output_string += \"**Keyword counts per category:**\\n\"\n",
        "    for cat, count in counts.items():\n",
        "        output_string += f\"- {cat.capitalize()}: {count}\\n\"\n",
        "\n",
        "    return output_string\n",
        "\n",
        "# Create the Gradio interface\n",
        "iface = gr.Interface(\n",
        "    fn=categorize_document,\n",
        "    inputs=gr.Textbox(lines=10, label=\"Enter your text document here:\", placeholder=\"Paste your text document here...\"),\n",
        "    outputs=gr.Markdown(label=\"Categorization Report\"), # Renamed label for clarity\n",
        "    title=\"Intelligent Text Categorizer\", # Updated title\n",
        "    description=\"This tool analyzes your text to identify its primary category (Sports, Politics, Technology, Finance, Health, Entertainment) based on keyword frequency. It also provides a detailed count of keywords found for each category after performing advanced text preprocessing like lowercasing, punctuation removal, tokenization, and lemmatization.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Launching Gradio app...\")\n",
        "    # You can set server_name='0.0.0.0' to make it accessible from other devices on your local network\n",
        "    # and server_port to specify a port if 7860 is busy.\n",
        "    iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "XM1YzkOZIkBg",
        "outputId": "b88ff7f2-1c38-4ca5-c72d-b35ca1e1fd23"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Gradio app...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://9f90473d9cb4e9040d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9f90473d9cb4e9040d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WORD CLOUD\n",
        "!pip install wordcloud matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gPwjDbyJkaX",
        "outputId": "8ac49fae-7c88-41d7-f041-1ace6b7e3ae3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WITH PREPROCESSING, GRADIO FRONTEND, AND WORD CLOUD GENERATION\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "import gradio as gr\n",
        "from wordcloud import WordCloud, STOPWORDS # Import WordCloud and STOPWORDS\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "\n",
        "# Download necessary NLTK data if you haven't already\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "except LookupError:\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Helper function to get the correct WordNet POS tag for lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map NLTK POS tag to WordNet POS tag for lemmatization\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Punctuation Removal (keep spaces)\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "\n",
        "    # 3. Tokenization: Split text into words using NLTK's word_tokenize\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # 4. Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "\n",
        "    return lemmatized_words\n",
        "\n",
        "# TEXT CATEGORY CLASSIFICATION FUNCTION\n",
        "def classify_text(text_words):\n",
        "    sports_keywords = {\n",
        "        \"athlete\", \"coach\", \"referee\", \"umpire\", \"tournament\", \"league\", \"championship\",\n",
        "        \"match\", \"competition\", \"victory\", \"defeat\", \"training\", \"practice\", \"stadium\",\n",
        "        \"arena\", \"court\", \"field\", \"track\", \"swimming\", \"running\", \"jumping\", \"throwing\",\n",
        "        \"cycling\", \"boxing\", \"wrestling\", \"gymnastics\", \"skiing\", \"snowboarding\", \"surfing\",\n",
        "        \"volleyball\", \"handball\", \"rugby\", \"cricket\", \"badminton\", \"table tennis\", \"golf\",\n",
        "        \"motorsport\", \"racing\", \"fishing\", \"hunting\", \"climbing\", \"hiking\", \"camping\",\n",
        "        \"outdoors\", \"adventure\", \"extreme sports\", \"fitness\", \"workout\", \"exercise\",\n",
        "        \"nutrition\", \"injury\", \"recovery\", \"physiotherapy\", \"sports medicine\",\n",
        "        \"sports science\", \"sports psychology\", \"sports management\", \"sports marketing\",\n",
        "        \"sports broadcasting\", \"sports photography\", \"sports writing\", \"sports history\",\n",
        "        \"sports statistics\", \"fantasy sports\", \"gambling\", \"betting\", \"esports\", \"gaming\"\n",
        "    }\n",
        "\n",
        "    technology_keywords = {\n",
        "        \"developer\", \"programmer\", \"coder\", \"engineer\", \"software\", \"web\", \"mobile\",\n",
        "        \"cloud\", \"data\", \"machine\", \"learning\", \"neural\", \"network\", \"computer\", \"vision\",\n",
        "        \"robotics\", \"automation\", \"internet\", \"blockchain\", \"cryptocurrency\", \"virtual\",\n",
        "        \"augmented\", \"cybersecurity\", \"security\", \"information\", \"technology\", \"telecommunications\",\n",
        "        \"wireless\", \"smartphone\", \"tablet\", \"laptop\", \"desktop\", \"server\", \"database\",\n",
        "        \"algorithm\", \"api\", \"framework\", \"library\", \"git\", \"docker\", \"kubernetes\", \"aws\",\n",
        "        \"azure\", \"cloud\", \"saas\", \"paas\", \"iaas\", \"fintech\", \"edtech\", \"healthtech\", \"biotech\",\n",
        "        \"quantum\", \"ethical\", \"privacy\", \"encryption\", \"firewall\", \"malware\", \"hacking\",\n",
        "        \"testing\", \"devops\", \"open\", \"source\", \"ux\", \"ui\", \"cto\", \"cio\"\n",
        "    }\n",
        "\n",
        "    politics_keywords = {\n",
        "        \"government\", \"election\", \"parliament\", \"senate\", \"congress\", \"president\",\n",
        "        \"minister\", \"party\", \"policy\", \"law\", \"bill\", \"vote\", \"campaign\", \"democracy\",\n",
        "        \"republic\", \"political\", \"politics\", \"politician\", \"treaty\", \"summit\", \"diplomacy\",\n",
        "        \"legislature\"\n",
        "    }\n",
        "\n",
        "    finance_keywords = {\n",
        "        \"finance\", \"financial\", \"economy\", \"economic\", \"market\", \"stock\", \"bond\",\n",
        "        \"investment\", \"investor\", \"currency\", \"exchange\", \"bank\", \"loan\", \"credit\",\n",
        "        \"debt\", \"income\", \"revenue\", \"profit\", \"loss\", \"tax\", \"budget\", \"audit\",\n",
        "        \"accounting\", \"insurance\", \"mortgage\", \"recession\", \"inflation\", \"interest\",\n",
        "        \"gdp\", \"fiscal\", \"monetary\"\n",
        "    }\n",
        "\n",
        "    health_keywords = {\n",
        "        \"health\", \"medical\", \"disease\", \"patient\", \"doctor\", \"hospital\", \"therapy\",\n",
        "        \"treatment\", \"symptom\", \"diagnosis\", \"illness\", \"wellness\", \"nutrition\",\n",
        "        \"exercise\", \"fitness\", \"medicine\", \"healthcare\", \"public\", \"epidemic\", \"pandemic\",\n",
        "        \"virus\", \"bacteria\", \"mental\", \"psychology\", \"pharmacy\", \"drug\", \"vaccine\",\n",
        "        \"clinic\", \"surgery\", \"recovery\"\n",
        "    }\n",
        "\n",
        "    entertainment_keywords = {\n",
        "        \"movie\", \"film\", \"show\", \"television\", \"series\", \"actor\", \"actress\", \"director\",\n",
        "        \"producer\", \"music\", \"song\", \"album\", \"artist\", \"concert\", \"festival\", \"theater\",\n",
        "        \"play\", \"comedy\", \"drama\", \"action\", \"thriller\", \"horror\", \"romance\",\n",
        "        \"animation\", \"documentary\", \"celebrity\", \"star\", \"award\", \"gala\", \"premiere\",\n",
        "        \"box\", \"streaming\", \"netflix\", \"hulu\", \"disney\", \"amazon\", \"youtube\", \"tiktok\",\n",
        "        \"instagram\", \"social\", \"media\", \"pop\", \"culture\", \"magazine\", \"interview\",\n",
        "        \"review\", \"critic\", \"fan\", \"fandom\", \"cosplay\", \"gaming\", \"esports\", \"twitch\"\n",
        "    }\n",
        "\n",
        "    all_keywords = {\n",
        "        \"sports\": sports_keywords,\n",
        "        \"politics\": politics_keywords,\n",
        "        \"technology\": technology_keywords,\n",
        "        \"health\": health_keywords,\n",
        "        \"finance\": finance_keywords,\n",
        "        \"entertainment\": entertainment_keywords\n",
        "    }\n",
        "\n",
        "    scores = {\n",
        "        \"sports\": 0,\n",
        "        \"politics\": 0,\n",
        "        \"technology\": 0,\n",
        "        \"health\": 0,\n",
        "        \"finance\": 0,\n",
        "        \"entertainment\": 0\n",
        "    }\n",
        "\n",
        "    for word in text_words:\n",
        "        for category_name, keywords_set in all_keywords.items():\n",
        "            if word in keywords_set:\n",
        "                scores[category_name] += 1\n",
        "\n",
        "    max_category = max(scores, key=scores.get)\n",
        "\n",
        "    if scores[max_category] == 0:\n",
        "        return \"unknown\", scores\n",
        "    return max_category, scores\n",
        "\n",
        "# Define the Gradio interface function\n",
        "def categorize_document_and_wordcloud(document_text):\n",
        "    # 1. Preprocess the input text (for classification)\n",
        "    processed_words_for_classification = preprocess_text(document_text)\n",
        "\n",
        "    # 2. Classify the preprocessed words\n",
        "    category, counts = classify_text(processed_words_for_classification)\n",
        "\n",
        "    # --- Word Cloud Generation ---\n",
        "\n",
        "    words_for_wordcloud_raw = document_text.lower()\n",
        "    words_for_wordcloud_cleaned = re.sub(r'[^a-z0-9\\s]', '', words_for_wordcloud_raw)\n",
        "\n",
        "    # Generate the word cloud\n",
        "    wordcloud = WordCloud(\n",
        "        width=800,\n",
        "        height=400,\n",
        "        background_color='white',\n",
        "        colormap='viridis',\n",
        "        stopwords=STOPWORDS, # Use WordCloud's built-in stopword filtering\n",
        "        collocations=False # Avoid grouping common phrases like \"New York\"\n",
        "    ).generate(words_for_wordcloud_cleaned) # Pass the single cleaned string here\n",
        "\n",
        "    # Plot the word cloud to a matplotlib figure\n",
        "    # Create a new figure object\n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    ax.imshow(wordcloud, interpolation='bilinear')\n",
        "    ax.axis('off') # Do not show axes\n",
        "    ax.set_title('Word Cloud of Your Document')\n",
        "\n",
        "    # Gradio's gr.Plot component expects a matplotlib Figure object.\n",
        "    # We explicitly return the 'fig' object.\n",
        "    wordcloud_figure = fig\n",
        "\n",
        "    # It's crucial to close the plot to prevent it from displaying in the backend/memory leak\n",
        "    # without affecting the Gradio frontend's display.\n",
        "    plt.close(fig) # Close the specific figure\n",
        "\n",
        "    # 4. Format the classification output for display in Gradio Markdown\n",
        "    output_classification_string = \"\"\n",
        "    if category == \"unknown\":\n",
        "        output_classification_string += \"**Classification Result:** Your text category is <span style='color: gray; font-weight: bold;'>UNKNOWN</span> (no specific keywords found).\\n\\n\"\n",
        "    else:\n",
        "        output_classification_string += f\"**Classification Result:** Your text is a <span style='color: blue; font-weight: bold;'>{category.upper()}</span> category.\\n\\n\"\n",
        "\n",
        "    output_classification_string += \"**Keyword counts per category:**\\n\"\n",
        "    for cat, count in counts.items():\n",
        "        output_classification_string += f\"- {cat.capitalize()}: {count}\\n\"\n",
        "\n",
        "    # Return both the Markdown string and the Matplotlib figure\n",
        "    return output_classification_string, wordcloud_figure\n",
        "\n",
        "# Create the Gradio interface with multiple outputs\n",
        "iface = gr.Interface(\n",
        "    fn=categorize_document_and_wordcloud,\n",
        "    inputs=gr.Textbox(lines=10, label=\"Enter your text document here:\", placeholder=\"Paste your text document here...\"),\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Categorization Report\"),\n",
        "        gr.Plot(label=\"Word Cloud\") # Use gr.Plot to display matplotlib figures\n",
        "    ],\n",
        "    title=\"Intelligent Text Categorizer & Word Cloud Generator\",\n",
        "    description=\"This tool analyzes your text to identify its primary category (Sports, Politics, Technology, Finance, Health, Entertainment) based on keyword frequency, provides detailed keyword counts, and generates a visual word cloud.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Launching Gradio app...\")\n",
        "    iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "mkT57z8sJ1ZW",
        "outputId": "3a6bdd25-044a-4c9b-a3f3-608a040580bc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Launching Gradio app...\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://21d2369177438ad32d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://21d2369177438ad32d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}